# ğŸ“„ RAG-Based PDF Question Answering (Google Colab)

This project demonstrates a **Retrieval-Augmented Generation (RAG)** pipeline implemented in **Google Colab**.
It allows users to load a PDF file, retrieve relevant document content using vector search, and generate answers using a language model.

The goal of this project is to clearly explain and experiment with **RAG fundamentals** using **LangChain, FAISS, and HuggingFace models**.

## ğŸ¯ Project Objective

To build a simple and understandable **RAG pipeline** that:

* Reads a PDF document
* Retrieves relevant text based on a user question
* Generates an answer grounded strictly in the document content

## âœ¨ Features

* ğŸ“‚ Load and process PDF documents
* âœ‚ï¸ Split text into manageable chunks
* ğŸ” Semantic search using vector embeddings
* ğŸ§  Retrieval-Augmented Generation (RAG)
* ğŸ¤– Answer generation using FLAN-T5
* âš¡ Runs entirely in Google Colab (CPU)

## ğŸ—ï¸ RAG Workflow Overview

```
PDF File
  â†“
Text Extraction
  â†“
Text Chunking
  â†“
Embedding Generation
  â†“
FAISS Vector Store
  â†“
Context Retrieval
  â†“
LLM Answer Generation
```

## ğŸ§  Step-by-Step Explanation

### 1ï¸âƒ£ Load PDF

The PDF file is loaded and converted into text using **PyPDFLoader**.

```python
loader = PyPDFLoader("Sample.pdf")
documents = loader.load()
```

### 2ï¸âƒ£ Split Text into Chunks

The document text is divided into overlapping chunks to preserve context.

```python
RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=100
)
```

### 3ï¸âƒ£ Generate Embeddings

Each text chunk is converted into a numerical vector using a **Sentence Transformer** model.

```python
sentence-transformers/all-MiniLM-L6-v2
```

### 4ï¸âƒ£ Store Embeddings in FAISS

FAISS enables fast similarity search over document embeddings.


### 5ï¸âƒ£ Retrieve Relevant Context

Given a question, FAISS retrieves the most relevant chunks from the document.

### 6ï¸âƒ£ Generate Answer (RAG)

The retrieved context and the userâ€™s question are passed to **FLAN-T5**, which generates an answer based **only on the document context**.

## ğŸ› ï¸ Tech Stack

* **Python**
* **LangChain**
* **FAISS**
* **HuggingFace Transformers**
* **Sentence-Transformers**
* **PyPDF**
* **Google Colab**

## ğŸš€ How to Run (Google Colab)

### 1ï¸âƒ£ Install Dependencies

```python
!pip install -U langchain-community langchain-text-splitters
!pip install transformers sentence-transformers faiss-cpu pypdf
```

### 2ï¸âƒ£ Upload PDF

Upload your PDF file (e.g., `Sample.pdf`) to the Colab environment.

### 3ï¸âƒ£ Run the Notebook Cells

Execute each cell sequentially to:

* Load the PDF
* Build the vector store
* Create the RAG chain
* Ask questions

### 4ï¸âƒ£ Ask a Question

```python
response = rag_chain.invoke({
    "question": "What is the goal of the AgriPredict system?"
})
print(response)
```

## ğŸ“ Repository Structure

```
.
â”œâ”€â”€ rag_pdf_qa_colab.ipynb   # Google Colab notebook
â”œâ”€â”€ Sample.pdf              # Example PDF
â”œâ”€â”€ README.md               # Project documentation
```

## ğŸ‘¤ Author

**Sowmya**
GitHub: [Sowmya](https://github.com/sowmya13531)

Just tell me ğŸ‘
